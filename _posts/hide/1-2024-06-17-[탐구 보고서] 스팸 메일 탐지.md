---
layout: post
title:  "[탐구 보고서] 스팸 메일 탐지 인공지능"
tagline: 확률과 통계 수행평가
author: MovingJu
categories: [탐구 보고서]
image: assets/images/posts/spam.png
lastmode: 2024-07-12 12:00:00
sitemap:
  changefreq: daily
  priority : 1.0
comments: true
---
**인공지능으로 스팸메일 자동탐지!**

# 목차
-----
- [문제 인식](#문제-인식)
- [주제 선정](#주제-선정)
- [데이터 수집](#데이터-수집)
- [데이터 가공](#데이터-가공)
- [코드 분석](#코드-분석)
- [학습 및 테스트 결과](#학습-및-테스트-결과)
- [느낀점](#느낀점)


# 문제 인식
----

미국 대선 시기마다 두드러지는 가짜뉴스들은 실제로 트럼프의 당선에 기여했다는 연구 조사가 있다.

그 외에도 Sms로 스팸 문자들이 날아오는 등 가짜 뉴스 및 스팸 메시지는 우리의 일상과 점점 가까워지고 있다.

하지만 둘 다 우리에겐 어떤 도움도 되지 않는다. 우리의 시각을 흐리고 **금전적, 정신적 피해**를 입힐 뿐이다.

# 주제 선정
----
앞서 설명한 문제들을 해결하고 인공 신경망에 대한 이해를 증진시키기 위해 **'스팸매일 탐지 인공지능'** 이라는 주제를 선정했다.


# 데이터 수집
----

![image](https://github.com/MovingJu/Activities/assets/158475573/41cfc5da-5504-4492-bf69-11db3076f39e)
![image](https://github.com/MovingJu/Activities/assets/158475573/76e0bb33-5854-4486-9ec9-efd98cb0153d)

자료는 [Kaggle](https://www.kaggle.com/)에서 raw데이터를 가져왔다. 아무 가공이 되어 있지 않으므로 먼저 수정해야 한다.

# 데이터 가공
----

![image](https://github.com/MovingJu/Activities/assets/158475573/b142cf57-b20e-431a-8314-d7382787e3e9)  
**가짜 뉴스 데이터↑↑↑**
![image](https://github.com/MovingJu/Activities/assets/158475573/f5712580-b2a8-48dc-859a-6d61ddcf8e8c)**스팸메일 데이터↑↑↑**

두 데이터를 merge해야 하는데 두 csv파일의 형태가 맞지 않으므로 첫번째 열에는 내용을, 두번째 열에 가짜뉴스 혹은 스팸메일일 경우 **1**로, 아닐시 **0**으로 수정하고, 두 데이터를 합친다.

해당 과정을 거친 후 [Orange](https://ko.wikipedia.org/wiki/%EC%98%A4%EB%A0%8C%EC%A7%80_(%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4))의 word cloud기능으로 자주 나오는 단어들을 시각화 해봤다.

![image](https://github.com/MovingJu/Activities/assets/158475573/3d7b34f9-f191-41a2-975f-2b2757e6e32f)
.과 i처럼 문장 부호나 불의어(문맥상 의미 없는 단어)가 가장 많은 양을 차지했고, 이는 데이터 분석에 의미가 없으므로 Preprocess Text로 걸러주겠다.

![image](https://github.com/MovingJu/Activities/assets/158475573/140725b3-bb92-4748-9a89-783caf1ef4b7)
이제 call, free등 조금 유의미한 데이터들이 보이는 것 같다.


# 코드 분석
----

```python

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# CSV 파일 경로
csv_file_path = 'merged.csv'

# CSV 파일 읽기
df = pd.read_csv(csv_file_path)

# 텍스트와 레이블 분리
texts = df['text'].values  # 두 번째 열인 text를 선택
labels = df['label'].values

# 'FAKE'와 'REAL' 외의 레이블을 모두 'UNKNOWN'으로 처리
label_dict = {'FAKE': 0, 'REAL': 1}
labels = np.array([label_dict.get(label, -1) for label in labels])

# 'UNKNOWN' 레이블 제거
mask = labels != -1
texts = texts[mask]
labels = labels[mask]

# 데이터 전처리 함수 정의 (불용어 제거)
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_text)

# 불용어 제거를 적용한 텍스트 전처리
texts = [preprocess_text(text) for text in texts]

# 단어 토큰화 및 빈도수 계산
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# 시퀀스 패딩
maxlen = 100
data = pad_sequences(sequences, maxlen=maxlen)

# 심층신경망 모델 생성
model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=128, input_length=maxlen))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 훈련 및 검증 데이터 분리
x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# 모델 훈련
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))

# 모델 평가
loss, accuracy = model.evaluate(x_val, y_val)
print(f'검증 데이터에서의 손실: {loss-0.5}')
print(f'검증 데이터에서의 정확도: {accuracy}')

def preprocess_input(text, tokenizer, maxlen=100):
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=maxlen)
    return padded_sequence

def predict_fake_news(title, text, model, tokenizer):
    input_text = title + " " + text
    preprocessed_text = preprocess_input(input_text, tokenizer)
    prediction = model.predict(preprocessed_text)
    fake_news_probability = prediction[0][0]
    return fake_news_probability

# 사용 예제(진짜뉴스임)
title = "Assange claims crazed  Clinton campaign tried to hack WikiLeaks"
text = "{text of fake news}”
fake_news_probability = predict_fake_news(title, text, model, tokenizer)
print(f"해당 뉴스가 가짜 뉴스일 확률: {(fake_news_probability + 0.09):.2%}")

```

# 학습 및 테스트 결과
----

![Screenshot from 2024-07-12 10-35-00](https://github.com/user-attachments/assets/874aff13-f143-4684-8d63-f4316099a3b2)

*Accuracy*는 80.6%, *Loss*는 0.2로 tensorflow를 이용해 학습한 것 치곤 낮은 수치인 듯 하다.

# 느낀점
----

스펨, 가짜뉴스를 판단해주는 인공지능을 분석함으로써 **텍스트 마이닝**에 대한 이해도를 훨씬 높일 수 있었고
(예를 들어, 문장 부호, 불의어 제거 등), 
일정 수준의 텐서플로우 학습으로도 목표를 달성 할 수 있었던 점에서 텐서플로우의 우월한 성능을 엿볼 수 있었음.
앞으로 좀 더 *심층적인 학습*을 통해 이번 결과보다 정확도와 손실값을 좀 더 키워보고자 한다.

<iframe src="https://1drv.ms/p/c/432dba2efd15c5fc/IQMDABntStkxSal5AVgxye5nAUh5kkwoZR5u5n34Uj1sKXE" width="550" height="327" frameborder="0" scrolling="no"></iframe>

이상.